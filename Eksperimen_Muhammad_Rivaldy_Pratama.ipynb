{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "# Import library untuk data manipulation dan analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import library untuk preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Import library untuk modeling\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import library untuk evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Import library untuk saving model\n",
        "import joblib\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3ItwTen_7E"
      },
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "# Download dataset Wine Quality dari UCI ML Repository\n",
        "# Dataset ini berisi informasi tentang kualitas wine berdasarkan karakteristik fisikokimia\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "df = pd.read_csv(url, sep=';')\n",
        "\n",
        "# Simpan dataset raw untuk backup\n",
        "os.makedirs('data', exist_ok=True)\n",
        "df.to_csv('data/winequality_raw.csv', index=False)\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\"*50)\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"First 5 rows:\")\n",
        "print(\"=\"*50)\n",
        "display(df.head())\n",
        "print(\"\\nâœ… Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 4.1 Informasi Dasar Dataset\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"INFORMASI DASAR DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah Baris: {df.shape[0]}\")\n",
        "print(f\"Jumlah Kolom: {df.shape[1]}\")\n",
        "print(f\"\\nNama Kolom:\\n{df.columns.tolist()}\")\n",
        "print(f\"\\nTipe Data:\\n{df.dtypes}\")\n",
        "\n",
        "# ========================================\n",
        "# 4.2 Statistik Deskriptif\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIK DESKRIPTIF\")\n",
        "print(\"=\"*60)\n",
        "display(df.describe())\n",
        "\n",
        "# ========================================\n",
        "# 4.3 Cek Missing Values\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "if missing_df['Missing Count'].sum() == 0:\n",
        "    print(\"âœ… Tidak ada missing values!\")\n",
        "\n",
        "# ========================================\n",
        "# 4.4 Cek Duplikasi Data\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DUPLIKASI DATA\")\n",
        "print(\"=\"*60)\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Jumlah data duplikat: {duplicates}\")\n",
        "print(f\"Persentase duplikasi: {(duplicates/len(df)*100):.2f}%\")\n",
        "\n",
        "# ========================================\n",
        "# 4.5 Distribusi Target Variable (Quality)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUSI TARGET VARIABLE (QUALITY)\")\n",
        "print(\"=\"*60)\n",
        "print(df['quality'].value_counts().sort_index())\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "df['quality'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title('Distribusi Kualitas Wine', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df['quality'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Proporsi Kualitas Wine', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.6 Distribusi Fitur Numerik\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUSI FITUR NUMERIK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualisasi histogram untuk semua fitur\n",
        "df.hist(bins=30, figsize=(20, 15), edgecolor='black', color='lightblue')\n",
        "plt.suptitle('Distribusi Semua Fitur', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.7 Correlation Matrix\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CORRELATION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix - Wine Quality Dataset', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Korelasi dengan target variable\n",
        "print(\"\\nKorelasi dengan Quality (diurutkan):\")\n",
        "quality_corr = correlation_matrix['quality'].sort_values(ascending=False)\n",
        "print(quality_corr)\n",
        "\n",
        "# ========================================\n",
        "# 4.8 Outlier Detection dengan Boxplot\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OUTLIER DETECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "features = df.columns[:-1]  # Semua kolom kecuali quality\n",
        "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(features):\n",
        "    axes[idx].boxplot(df[col], vert=True, patch_artist=True,\n",
        "                     boxprops=dict(facecolor='lightblue', color='black'),\n",
        "                     medianprops=dict(color='red', linewidth=2))\n",
        "    axes[idx].set_title(f'{col}', fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Boxplot untuk Deteksi Outlier', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.9 Relationship antara Fitur dengan Quality\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RELATIONSHIP FITUR DENGAN QUALITY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih 4 fitur dengan korelasi tertinggi\n",
        "top_features = quality_corr[1:5].index.tolist()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(top_features):\n",
        "    axes[idx].scatter(df[feature], df['quality'], alpha=0.5, c=df['quality'], \n",
        "                     cmap='viridis', edgecolors='black', linewidth=0.5)\n",
        "    axes[idx].set_xlabel(feature, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Quality', fontweight='bold')\n",
        "    axes[idx].set_title(f'{feature} vs Quality\\n(Correlation: {quality_corr[feature]:.3f})', \n",
        "                       fontweight='bold')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Exploratory Data Analysis completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf8KUPXLg5r"
      },
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
        "\n",
        "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 5.1 Handling Duplicate Data\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: HANDLING DUPLICATE DATA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah data sebelum menghapus duplikat: {len(df)}\")\n",
        "df_clean = df.drop_duplicates()\n",
        "print(f\"Jumlah data setelah menghapus duplikat: {len(df_clean)}\")\n",
        "print(f\"Jumlah duplikat yang dihapus: {len(df) - len(df_clean)}\")\n",
        "\n",
        "# ========================================\n",
        "# 5.2 Feature Engineering - Binning Quality\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: FEATURE ENGINEERING - BINNING QUALITY\")\n",
        "print(\"=\"*60)\n",
        "# Konversi quality menjadi 3 kategori: Low (3-5), Medium (6), High (7-8)\n",
        "def categorize_quality(quality):\n",
        "    if quality <= 5:\n",
        "        return 0  # Low\n",
        "    elif quality == 6:\n",
        "        return 1  # Medium\n",
        "    else:\n",
        "        return 2  # High\n",
        "\n",
        "df_clean['quality_category'] = df_clean['quality'].apply(categorize_quality)\n",
        "\n",
        "print(\"Distribusi Quality Category:\")\n",
        "print(df_clean['quality_category'].value_counts().sort_index())\n",
        "print(\"\\nMapping:\")\n",
        "print(\"0 = Low Quality (3-5)\")\n",
        "print(\"1 = Medium Quality (6)\")\n",
        "print(\"2 = High Quality (7-8)\")\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "df_clean['quality'].hist(bins=6, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribusi Quality (Original)', fontweight='bold')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df_clean['quality_category'].value_counts().sort_index().plot(kind='bar', color='coral', edgecolor='black')\n",
        "plt.title('Distribusi Quality Category (Binned)', fontweight='bold')\n",
        "plt.xlabel('Category (0=Low, 1=Medium, 2=High)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 5.3 Handling Outliers menggunakan IQR Method\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: HANDLING OUTLIERS (IQR METHOD)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def remove_outliers_iqr(data, columns):\n",
        "    df_no_outliers = data.copy()\n",
        "    outliers_removed = {}\n",
        "    \n",
        "    for col in columns:\n",
        "        Q1 = df_no_outliers[col].quantile(0.25)\n",
        "        Q3 = df_no_outliers[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        before = len(df_no_outliers)\n",
        "        df_no_outliers = df_no_outliers[\n",
        "            (df_no_outliers[col] >= lower_bound) & \n",
        "            (df_no_outliers[col] <= upper_bound)\n",
        "        ]\n",
        "        after = len(df_no_outliers)\n",
        "        outliers_removed[col] = before - after\n",
        "    \n",
        "    return df_no_outliers, outliers_removed\n",
        "\n",
        "# Kolom yang akan dibersihkan dari outlier\n",
        "numeric_cols = df_clean.columns[:-2].tolist()  # Exclude quality dan quality_category\n",
        "\n",
        "print(f\"Jumlah data sebelum menghapus outlier: {len(df_clean)}\")\n",
        "df_no_outliers, outliers_info = remove_outliers_iqr(df_clean, numeric_cols)\n",
        "print(f\"Jumlah data setelah menghapus outlier: {len(df_no_outliers)}\")\n",
        "print(f\"\\nOutliers dihapus per kolom:\")\n",
        "for col, count in outliers_info.items():\n",
        "    if count > 0:\n",
        "        print(f\"  - {col}: {count} outliers\")\n",
        "\n",
        "# ========================================\n",
        "# 5.4 Feature Scaling - Standardization\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: FEATURE SCALING (STANDARDIZATION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pisahkan features dan target\n",
        "X = df_no_outliers.drop(['quality', 'quality_category'], axis=1)\n",
        "y = df_no_outliers['quality_category']\n",
        "\n",
        "print(f\"Shape of Features (X): {X.shape}\")\n",
        "print(f\"Shape of Target (y): {y.shape}\")\n",
        "print(f\"\\nTarget distribution:\\n{y.value_counts().sort_index()}\")\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(\"\\nâœ… Features scaled successfully!\")\n",
        "print(\"\\nStatistik setelah scaling:\")\n",
        "display(X_scaled_df.describe())\n",
        "\n",
        "# Visualisasi perbandingan sebelum dan sesudah scaling\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Before scaling\n",
        "axes[0].boxplot([X[col] for col in X.columns], labels=X.columns, patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightblue'))\n",
        "axes[0].set_title('Before Scaling', fontweight='bold', fontsize=14)\n",
        "axes[0].set_ylabel('Value')\n",
        "axes[0].tick_params(axis='x', rotation=90)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# After scaling\n",
        "axes[1].boxplot([X_scaled_df[col] for col in X_scaled_df.columns], labels=X_scaled_df.columns, \n",
        "                patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
        "axes[1].set_title('After Scaling', fontweight='bold', fontsize=14)\n",
        "axes[1].set_ylabel('Standardized Value')\n",
        "axes[1].tick_params(axis='x', rotation=90)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 5.5 Train-Test Split\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTarget distribution in training set:\\n{y_train.value_counts().sort_index()}\")\n",
        "print(f\"\\nTarget distribution in testing set:\\n{y_test.value_counts().sort_index()}\")\n",
        "\n",
        "# ========================================\n",
        "# 5.6 Save Preprocessed Data\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: SAVE PREPROCESSED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.makedirs('data/preprocessed', exist_ok=True)\n",
        "\n",
        "# Gabungkan kembali X dan y untuk disimpan\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "test_data = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "train_data.to_csv('data/preprocessed/train_data.csv', index=False)\n",
        "test_data.to_csv('data/preprocessed/test_data.csv', index=False)\n",
        "\n",
        "# Simpan scaler\n",
        "joblib.dump(scaler, 'data/preprocessed/scaler.pkl')\n",
        "\n",
        "print(\"âœ… Preprocessed data saved successfully!\")\n",
        "print(f\"  - Train data: data/preprocessed/train_data.csv\")\n",
        "print(f\"  - Test data: data/preprocessed/test_data.csv\")\n",
        "print(f\"  - Scaler: data/preprocessed/scaler.pkl\")\n",
        "\n",
        "# ========================================\n",
        "# 5.7 Summary\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ… Original data: {len(df)} samples\")\n",
        "print(f\"âœ… After removing duplicates: {len(df_clean)} samples\")\n",
        "print(f\"âœ… After removing outliers: {len(df_no_outliers)} samples\")\n",
        "print(f\"âœ… Training samples: {len(X_train)}\")\n",
        "print(f\"âœ… Testing samples: {len(X_test)}\")\n",
        "print(f\"âœ… Number of features: {X_train.shape[1]}\")\n",
        "print(f\"âœ… Number of classes: {len(y.unique())}\")\n",
        "print(\"\\nðŸŽ‰ Data Preprocessing Completed Successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
