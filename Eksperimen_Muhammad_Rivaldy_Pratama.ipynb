{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n",
        "\n",
        "## **Heart Disease Dataset (Cleveland)**\n",
        "\n",
        "Dataset yang digunakan dalam eksperimen ini adalah **Heart Disease Dataset** dari **UCI Machine Learning Repository**. Dataset ini merupakan salah satu dataset medis yang paling populer untuk klasifikasi penyakit jantung.\n",
        "\n",
        "### **Deskripsi Dataset:**\n",
        "- **Sumber**: UCI Machine Learning Repository - Cleveland Heart Disease Database\n",
        "- **URL**: https://archive.ics.uci.edu/ml/datasets/heart+Disease\n",
        "- **Jumlah Sampel**: 303 pasien\n",
        "- **Jumlah Fitur**: 13 fitur medis + 1 target variable\n",
        "- **Tipe Problem**: Binary Classification (Ada penyakit jantung atau tidak)\n",
        "- **Target Variable**: \n",
        "  - `0` = No disease (Tidak ada penyakit jantung)\n",
        "  - `1` = Disease present (Ada penyakit jantung)\n",
        "\n",
        "### **Fitur-Fitur dalam Dataset:**\n",
        "\n",
        "1. **age**: Usia pasien (dalam tahun)\n",
        "2. **sex**: Jenis kelamin (1 = laki-laki, 0 = perempuan)\n",
        "3. **cp**: Tipe nyeri dada (chest pain type)\n",
        "   - 0: Typical angina\n",
        "   - 1: Atypical angina\n",
        "   - 2: Non-anginal pain\n",
        "   - 3: Asymptomatic\n",
        "4. **trestbps**: Tekanan darah saat istirahat (mm Hg)\n",
        "5. **chol**: Kolesterol serum (mg/dl)\n",
        "6. **fbs**: Gula darah puasa > 120 mg/dl (1 = true, 0 = false)\n",
        "7. **restecg**: Hasil elektrokardiografi saat istirahat\n",
        "   - 0: Normal\n",
        "   - 1: Abnormalitas gelombang ST-T\n",
        "   - 2: Left ventricular hypertrophy\n",
        "8. **thalach**: Denyut jantung maksimum yang dicapai\n",
        "9. **exang**: Exercise induced angina (1 = yes, 0 = no)\n",
        "10. **oldpeak**: ST depression induced by exercise relative to rest\n",
        "11. **slope**: Kemiringan segmen ST saat exercise\n",
        "    - 0: Upsloping\n",
        "    - 1: Flat\n",
        "    - 2: Downsloping\n",
        "12. **ca**: Jumlah pembuluh darah utama (0-3) yang diwarnai fluoroscopy\n",
        "13. **thal**: Thalassemia\n",
        "    - 1: Normal\n",
        "    - 2: Fixed defect\n",
        "    - 3: Reversible defect\n",
        "\n",
        "### **Tujuan Eksperimen:**\n",
        "Membangun model machine learning untuk **memprediksi apakah seseorang memiliki penyakit jantung atau tidak** berdasarkan 13 fitur medis di atas.\n",
        "\n",
        "### **Kenapa Dataset Ini Dipilih?**\n",
        "- Dataset medis yang memiliki aplikasi real-world yang penting\n",
        "- Ukuran dataset moderat (303 samples) - cocok untuk eksperimen\n",
        "- Binary classification - problem klasik yang mudah dipahami\n",
        "- Memiliki missing values - memberikan kesempatan untuk praktik data cleaning\n",
        "- Tersedia secara publik di UCI ML Repository\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "Dataset diperoleh dari **UCI Machine Learning Repository**, salah satu repositori data machine learning terbesar dan paling terpercaya. Dataset Heart Disease (Cleveland) ini telah digunakan dalam berbagai penelitian ilmiah dan merupakan benchmark standar untuk klasifikasi penyakit jantung.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Library yang digunakan mencakup pandas untuk manipulasi data, numpy untuk operasi numerik, matplotlib dan seaborn untuk visualisasi, serta scikit-learn untuk preprocessing dan modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "# Import library untuk data manipulation dan analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import library untuk preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Import library untuk modeling\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import library untuk evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Import library untuk saving model\n",
        "import joblib\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3ItwTen_7E"
      },
      "source": [
        "Dataset Heart Disease akan dimuat langsung dari UCI ML Repository menggunakan pandas. Data akan diload dengan handling missing values (yang ditandai dengan '?'), kemudian dilakukan konversi target menjadi binary classification untuk memudahkan pemodelan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "# Load dataset Heart Disease dari file raw\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load dataset dari file yang sudah ada\n",
        "df = pd.read_csv('../heart_disease_raw.csv')\n",
        "\n",
        "# Convert target menjadi binary classification jika belum\n",
        "# 0 = no disease, 1-4 = disease present -> 0 = no disease, 1 = disease\n",
        "df['target'] = (df['target'] > 0).astype(int)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HEART DISEASE DATASET - LOADING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
        "print(f\"üìù Number of samples: {df.shape[0]}\")\n",
        "print(f\"üìù Number of features: {df.shape[1] - 1} (+ 1 target)\")\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Informasi Dasar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tampilkan informasi dasar dataset\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nColumn Names & Data Types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Total Rows: {len(df)}\")\n",
        "print(f\"Total Columns: {len(df.columns)}\")\n",
        "print(f\"\\nColumn Names: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Distribusi Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisis distribusi target variable\n",
        "print(\"=\"*70)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nTarget Value Counts:\")\n",
        "print(df['target'].value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n   - Class 0 (No Disease): {(df['target']==0).sum()} patients ({(df['target']==0).sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"   - Class 1 (Disease):    {(df['target']==1).sum()} patients ({(df['target']==1).sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Visualisasi distribusi target\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['target'].value_counts().plot(kind='bar', color=['lightblue', 'lightcoral'])\n",
        "plt.title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Target (0=No Disease, 1=Disease)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks([0, 1], ['No Disease', 'Disease'], rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Preview Data (Head & Tail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tampilkan 5 baris pertama\n",
        "print(\"=\"*70)\n",
        "print(\"FIRST 5 ROWS OF DATASET\")\n",
        "print(\"=\"*70)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Exploratory Data Analysis dilakukan untuk memahami karakteristik dataset secara mendalam. Tahap ini mencakup analisis statistik deskriptif, distribusi data, korelasi antar fitur, deteksi outlier, dan hubungan fitur dengan target variable. EDA sangat penting untuk menentukan langkah preprocessing yang tepat dan memahami pola dalam data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 4.1 Informasi Dasar Dataset\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"INFORMASI DASAR DATASET - HEART DISEASE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah Baris: {df.shape[0]}\")\n",
        "print(f\"Jumlah Kolom: {df.shape[1]}\")\n",
        "print(f\"\\nNama Kolom:\\n{df.columns.tolist()}\")\n",
        "print(f\"\\nTipe Data:\\n{df.dtypes}\")\n",
        "\n",
        "# ========================================\n",
        "# 4.2 Statistik Deskriptif\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIK DESKRIPTIF\")\n",
        "print(\"=\"*60)\n",
        "display(df.describe())\n",
        "\n",
        "# ========================================\n",
        "# 4.3 Cek Missing Values\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "if missing_df['Missing Count'].sum() == 0:\n",
        "    print(\"‚úÖ Tidak ada missing values!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Total missing values: {missing_df['Missing Count'].sum()}\")\n",
        "\n",
        "# ========================================\n",
        "# 4.4 Cek Duplikasi Data\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DUPLIKASI DATA\")\n",
        "print(\"=\"*60)\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Jumlah data duplikat: {duplicates}\")\n",
        "print(f\"Persentase duplikasi: {(duplicates/len(df)*100):.2f}%\")\n",
        "\n",
        "# ========================================\n",
        "# 4.5 Distribusi Target Variable (Heart Disease)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUSI TARGET VARIABLE (HEART DISEASE)\")\n",
        "print(\"=\"*60)\n",
        "print(df['target'].value_counts().sort_index())\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "df['target'].value_counts().sort_index().plot(kind='bar', color=['skyblue', 'salmon'], edgecolor='black')\n",
        "plt.title('Distribusi Heart Disease', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Disease Status (0=No Disease, 1=Disease)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.xticks([0, 1], ['No Disease', 'Disease'])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df['target'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%', startangle=90, \n",
        "                                              colors=['skyblue', 'salmon'])\n",
        "plt.title('Proporsi Heart Disease', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('')\n",
        "plt.legend(['No Disease', 'Disease'], loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.6 Distribusi Fitur Numerik\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUSI FITUR NUMERIK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualisasi histogram untuk semua fitur\n",
        "df.hist(bins=20, figsize=(20, 12), edgecolor='black', color='lightblue')\n",
        "plt.suptitle('Distribusi Semua Fitur - Heart Disease Dataset', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.7 Correlation Matrix\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CORRELATION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix - Heart Disease Dataset', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Korelasi dengan target variable\n",
        "print(\"\\nKorelasi dengan Target (Heart Disease) - diurutkan:\")\n",
        "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
        "print(target_corr)\n",
        "\n",
        "# ========================================\n",
        "# 4.8 Outlier Detection dengan Boxplot\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OUTLIER DETECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "features = df.columns[:-1]  # Semua kolom kecuali target\n",
        "fig, axes = plt.subplots(4, 4, figsize=(18, 16))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(features):\n",
        "    if idx < len(axes):\n",
        "        axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                         boxprops=dict(facecolor='lightblue', color='black'),\n",
        "                         medianprops=dict(color='red', linewidth=2))\n",
        "        axes[idx].set_title(f'{col}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Value')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide extra subplots\n",
        "for idx in range(len(features), len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.suptitle('Boxplot untuk Deteksi Outlier - Heart Disease', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 4.9 Relationship antara Fitur dengan Target\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RELATIONSHIP FITUR DENGAN TARGET (HEART DISEASE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih 4 fitur dengan korelasi tertinggi (exclude target itu sendiri)\n",
        "top_features = target_corr[1:5].index.tolist()\n",
        "print(f\"Top 4 features dengan korelasi tertinggi: {top_features}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(top_features):\n",
        "    axes[idx].scatter(df[feature], df['target'], alpha=0.5, c=df['target'], \n",
        "                     cmap='coolwarm', edgecolors='black', linewidth=0.5, s=50)\n",
        "    axes[idx].set_xlabel(feature, fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_ylabel('Target (0=No Disease, 1=Disease)', fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_title(f'{feature} vs Heart Disease\\n(Correlation: {target_corr[feature]:.3f})', \n",
        "                       fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_yticks([0, 1])\n",
        "    axes[idx].set_yticklabels(['No Disease', 'Disease'])\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Exploratory Data Analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COf8KUPXLg5r"
      },
      "source": [
        "Data preprocessing merupakan langkah krusial untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering mengandung missing values, duplikasi, atau rentang nilai yang tidak konsisten. Proses ini bertujuan membersihkan dan mempersiapkan data agar siap untuk pemodelan.\n",
        "\n",
        "Tahapan preprocessing yang dilakukan meliputi:\n",
        "1. **Handling Missing Values** - Mengisi nilai yang hilang dengan strategi median imputation\n",
        "2. **Removing Duplicates** - Menghapus data duplikat untuk menghindari bias\n",
        "3. **Feature Scaling** - Standardisasi fitur menggunakan StandardScaler\n",
        "4. **Train-Test Split** - Membagi data dengan stratified sampling untuk menjaga proporsi kelas\n",
        "5. **Data Saving** - Menyimpan data yang sudah diproses untuk tahap modeling\n",
        "\n",
        "Setiap langkah disesuaikan dengan karakteristik dataset Heart Disease yang merupakan data terstruktur dengan fitur numerik.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 5.1 Handling Missing Values\n",
        "# ========================================\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: HANDLING MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Missing values sebelum handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Impute missing values dengan median untuk kolom numerik\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Get columns with missing values\n",
        "cols_with_missing = df.columns[df.isnull().any()].tolist()\n",
        "print(f\"\\nKolom dengan missing values: {cols_with_missing}\")\n",
        "\n",
        "if cols_with_missing:\n",
        "    df[cols_with_missing] = imputer.fit_transform(df[cols_with_missing])\n",
        "    print(f\"\\n‚úÖ Missing values telah diisi dengan median\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Tidak ada missing values\")\n",
        "\n",
        "print(f\"\\nMissing values setelah handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ========================================\n",
        "# 5.2 Handling Duplicate Data\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: HANDLING DUPLICATE DATA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah data sebelum menghapus duplikat: {len(df)}\")\n",
        "df_clean = df.drop_duplicates()\n",
        "print(f\"Jumlah data setelah menghapus duplikat: {len(df_clean)}\")\n",
        "print(f\"Jumlah duplikat yang dihapus: {len(df) - len(df_clean)}\")\n",
        "\n",
        "# ========================================\n",
        "# 5.3 Visualisasi Distribusi Target\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: DISTRIBUSI TARGET VARIABLE\")\n",
        "print(\"=\"*60)\n",
        "print(\"Distribusi Heart Disease (Binary Classification):\")\n",
        "print(df_clean['target'].value_counts().sort_index())\n",
        "print(f\"\\nClass Balance:\")\n",
        "print(f\"  - No Disease (0): {(df_clean['target']==0).sum()} ({(df_clean['target']==0).sum()/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"  - Disease (1):    {(df_clean['target']==1).sum()} ({(df_clean['target']==1).sum()/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "df_clean['target'].value_counts().sort_index().plot(kind='bar', color=['lightgreen', 'coral'], edgecolor='black')\n",
        "plt.title('Distribusi Heart Disease (Binary)', fontweight='bold')\n",
        "plt.xlabel('Target (0=No Disease, 1=Disease)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks([0, 1], ['No Disease', 'Disease'], rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df_clean['target'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%', \n",
        "                                                     colors=['lightgreen', 'coral'], startangle=90)\n",
        "plt.title('Proporsi Heart Disease', fontweight='bold')\n",
        "plt.ylabel('')\n",
        "plt.legend(['No Disease', 'Disease'], loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 5.4 Feature Scaling - Standardization\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: FEATURE SCALING (STANDARDIZATION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pisahkan features dan target\n",
        "X = df_clean.drop('target', axis=1)\n",
        "y = df_clean['target']\n",
        "\n",
        "print(f\"Shape of Features (X): {X.shape}\")\n",
        "print(f\"Shape of Target (y): {y.shape}\")\n",
        "print(f\"\\nFeatures: {X.columns.tolist()}\")\n",
        "print(f\"\\nTarget distribution:\\n{y.value_counts().sort_index()}\")\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(\"\\n‚úÖ Features scaled successfully!\")\n",
        "print(\"\\nStatistik setelah scaling (mean ‚âà 0, std ‚âà 1):\")\n",
        "display(X_scaled_df.describe())\n",
        "\n",
        "# Visualisasi perbandingan sebelum dan sesudah scaling\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Before scaling\n",
        "axes[0].boxplot([X[col] for col in X.columns[:6]], labels=X.columns[:6], patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightblue'))\n",
        "axes[0].set_title('Before Scaling (Sample 6 Features)', fontweight='bold', fontsize=14)\n",
        "axes[0].set_ylabel('Value')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# After scaling\n",
        "axes[1].boxplot([X_scaled_df[col] for col in X_scaled_df.columns[:6]], labels=X_scaled_df.columns[:6], \n",
        "                patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
        "axes[1].set_title('After Scaling (Sample 6 Features)', fontweight='bold', fontsize=14)\n",
        "axes[1].set_ylabel('Standardized Value')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# 5.5 Train-Test Split\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: TRAIN-TEST SPLIT (STRATIFIED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTarget distribution in training set:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"  - No Disease: {(y_train==0).sum()} ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"  - Disease:    {(y_train==1).sum()} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"\\nTarget distribution in testing set:\")\n",
        "print(y_test.value_counts().sort_index())\n",
        "print(f\"  - No Disease: {(y_test==0).sum()} ({(y_test==0).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"  - Disease:    {(y_test==1).sum()} ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# ========================================\n",
        "# 5.6 Save Preprocessed Data\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: SAVE PREPROCESSED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.makedirs('data/preprocessed', exist_ok=True)\n",
        "\n",
        "# Gabungkan kembali X dan y untuk disimpan\n",
        "train_data = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
        "test_data = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "train_data.to_csv('data/preprocessed/train_data.csv', index=False)\n",
        "test_data.to_csv('data/preprocessed/test_data.csv', index=False)\n",
        "\n",
        "# Simpan scaler\n",
        "joblib.dump(scaler, 'data/preprocessed/scaler.pkl')\n",
        "\n",
        "print(\"‚úÖ Preprocessed data saved successfully!\")\n",
        "print(f\"  - Train data: data/preprocessed/train_data.csv\")\n",
        "print(f\"    Shape: {train_data.shape}, Size: {os.path.getsize('data/preprocessed/train_data.csv')/1024:.2f} KB\")\n",
        "print(f\"  - Test data: data/preprocessed/test_data.csv\")\n",
        "print(f\"    Shape: {test_data.shape}, Size: {os.path.getsize('data/preprocessed/test_data.csv')/1024:.2f} KB\")\n",
        "print(f\"  - Scaler: data/preprocessed/scaler.pkl\")\n",
        "\n",
        "# ========================================\n",
        "# 5.7 Summary\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY - HEART DISEASE DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Original data: {len(df)} samples\")\n",
        "print(f\"‚úÖ After handling missing values: {len(df)} samples\")\n",
        "print(f\"‚úÖ After removing duplicates: {len(df_clean)} samples\")\n",
        "print(f\"‚úÖ Training samples: {len(X_train)} ({len(X_train)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Testing samples: {len(X_test)} ({len(X_test)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Number of features: {X_train.shape[1]}\")\n",
        "print(f\"‚úÖ Number of classes: {len(y.unique())} (Binary Classification)\")\n",
        "print(f\"‚úÖ Feature scaling: StandardScaler (mean‚âà0, std‚âà1)\")\n",
        "print(f\"‚úÖ Train-test split: Stratified (maintains class balance)\")\n",
        "print(\"\\nüéâ Data Preprocessing Completed Successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
